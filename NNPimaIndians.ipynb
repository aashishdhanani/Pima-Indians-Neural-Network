{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np #this is the math component\n",
    "\n",
    "#pytorch is library that helps develop deep learning models\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac5fc7f",
   "metadata": {},
   "source": [
    "Data is the Pima Indians onset of diabetes which describes patient medical record for Pima Indians and whether or not they had an onset of diabetes within 5 years"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affd6ff8",
   "metadata": {},
   "source": [
    "# Loading Data\n",
    "\n",
    "This is a binary classification problem (either 0 or 1). There are 8 input variables:\n",
    "\n",
    "1. Number of times pregnant\n",
    "2. Plasma glucose concentration at 2 hours in an oral glucose tolerance test\n",
    "3. Diastolic blood pressure (mm Hg)\n",
    "4. Triceps skin fold thickness (mm)\n",
    "5. 2-hour serum insulin (Î¼IU/ml)\n",
    "6. Body mass index (weight in kg/(height in m)2)\n",
    "7. Diabetes pedigree function\n",
    "8. Age (years)\n",
    "\n",
    "\n",
    "Note that the data that I am using is already cleaned. Usually, you would take dataset and use feature engineering to make it usuable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1727728d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the dataset, split into input (X) and output (Y) variables\n",
    "dataset = np.loadtxt('pima-indians-diabetes.data.csv', delimiter= ',')\n",
    "X = dataset[:,0:8]\n",
    "Y = dataset[:,8]\n",
    "\n",
    "#data should be converted to pytorch tensors first. Helps avoid the implicit conversion between 64-bit and 32-bit floating point\n",
    "\n",
    "X = torch.tensor(X, dtype=torch.float32)\n",
    "Y = torch.tensor(Y, dtype=torch.float32).reshape(-1,1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dcc9464",
   "metadata": {},
   "source": [
    "# Define the Model\n",
    "\n",
    "A model can be defined as a sequence of layers. You create a SEQUENTIAL model with the layers listed out. First thing to do is to ensure that the first layer has the correct number of input features (8 for this example). Often, the best NN structure is found through trial and error. In this example, will be using 3 layers. \n",
    "\n",
    "Fully connected layers are defined using the LINEAR class in pytorch. Simply means an operation similar to matrix multiplication. Specifiy the number of inputs as the first argument and the number of outputs as the second argument. Number of outputs are called the number of neurons or number of nodes in the layer.\n",
    "\n",
    "Need activation function after the layer. If not provided, you take output of the matrix multiplication to next step, or sometimes call it using linear activation. In this example, we will use ReLU (rectified linear unit activation function) on the first two layers and the sigmoid function in the output layer.\n",
    "\n",
    "A sigmoid on the output layer ensures the output is between 0 and 1. Note that using sigmoid can lead to the problem of vanishing gradient in deep NN, and ReLU activation is found to provide better performance in terms of speed and accuracy.\n",
    "\n",
    "- The model expects rows of data with 8 variables\n",
    "- The first hidden layer has 12 neurons, followed by ReLU function\n",
    "- The second hidden layer has 8 neurons, followed by ReLU function\n",
    "- Output layer has 1 neuron, followed by sigmoid function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2bf18a29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PimaClassifier(\n",
      "  (hidden1): Linear(in_features=8, out_features=12, bias=True)\n",
      "  (act1): ReLU()\n",
      "  (hidden2): Linear(in_features=12, out_features=8, bias=True)\n",
      "  (act2): ReLU()\n",
      "  (output): Linear(in_features=8, out_features=1, bias=True)\n",
      "  (act_output): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# model = nn.Sequential(\n",
    "#     nn.Linear(8,12),\n",
    "#     nn.ReLU(),\n",
    "#     nn.Linear(12,8),\n",
    "#     nn.ReLU(),\n",
    "#     nn.Linear(8,1),\n",
    "#     nn.Sigmoid()\n",
    "# )\n",
    "\n",
    "# print(model)\n",
    "class PimaClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.hidden1 = nn.Linear(8, 12)\n",
    "        self.act1 = nn.ReLU()\n",
    "        self.hidden2 = nn.Linear(12, 8)\n",
    "        self.act2 = nn.ReLU()\n",
    "        self.output = nn.Linear(8, 1)\n",
    "        self.act_output = nn.Sigmoid()\n",
    " \n",
    "    def forward(self, x):\n",
    "        x = self.act1(self.hidden1(x))\n",
    "        x = self.act2(self.hidden2(x))\n",
    "        x = self.act_output(self.output(x))\n",
    "        return x\n",
    " \n",
    "model = PimaClassifier()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e20b572b",
   "metadata": {},
   "source": [
    "# Preparation for Training\n",
    "\n",
    "A defined model is ready, but still need to specify what the goal of the training is. Training a NN means finding the best set of weights to map inputs to outputs in your dataset. The loss function is the metric to measure predictions. In this example, since the project is a binary classification, we will use binary cross entropy. \n",
    "\n",
    "You also need an optimizer, the algorithm you use to adjust the model weights to produce better outputs. We will use Adam, which is a popular version of gradient descent and can automatically tune itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dc8bc11e",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.BCELoss() #binary cross entropy\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "#lr is the learning rate which is a config parameter for the optimizer. You pass it on model.parameters which is a generator of all parameters from the model created"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f97def5",
   "metadata": {},
   "source": [
    "# Training a Model\n",
    "\n",
    "Training a NN takes in epochs and batches\n",
    "\n",
    "EPOCH - Passes the entire training dataset to the model once\n",
    "\n",
    "BATCH - one or more samples passed to the model, from which the gradient descent algorithm will be executed for one iteration\n",
    "\n",
    "The entire dataset is split into batches and you pass the batches one by one into a model using a training loop. Once you have exhausted all batches, you have finished 1 epoch. Then you can start over and refine the model.\n",
    "\n",
    "The size of the batch is limited by the system's memory. The number of computations required is linearly proportional to the size of the batch. The total number of batches over many epochs is how many times you run the GD to refine the model. Note that it is a tradeoff that you want more iterations for the GD so you can produced a better model, but at the same time, you do not want the training to take too long to complete.\n",
    "\n",
    "The goal of training a model is to ensure it learns a good enough mappying of input data to output classification. It won't be perfect. You will see the amount of error reducing when it the later epochs, it will level out, known as model convergence.\n",
    "\n",
    "Simplest way is to use 2 nested loops, one for epochs and one for batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b1f03678",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch 0, latest loss 0.6136674284934998\n",
      "Finished epoch 1, latest loss 0.5868992209434509\n",
      "Finished epoch 2, latest loss 0.5670740008354187\n",
      "Finished epoch 3, latest loss 0.5466744303703308\n",
      "Finished epoch 4, latest loss 0.5284450650215149\n",
      "Finished epoch 5, latest loss 0.508657693862915\n",
      "Finished epoch 6, latest loss 0.48628512024879456\n",
      "Finished epoch 7, latest loss 0.45772385597229004\n",
      "Finished epoch 8, latest loss 0.4490693211555481\n",
      "Finished epoch 9, latest loss 0.4439621567726135\n",
      "Finished epoch 10, latest loss 0.4399668574333191\n",
      "Finished epoch 11, latest loss 0.4279786944389343\n",
      "Finished epoch 12, latest loss 0.42225590348243713\n",
      "Finished epoch 13, latest loss 0.4138154685497284\n",
      "Finished epoch 14, latest loss 0.41570693254470825\n",
      "Finished epoch 15, latest loss 0.4139489531517029\n",
      "Finished epoch 16, latest loss 0.4115311801433563\n",
      "Finished epoch 17, latest loss 0.4028107523918152\n",
      "Finished epoch 18, latest loss 0.41689756512641907\n",
      "Finished epoch 19, latest loss 0.39571043848991394\n",
      "Finished epoch 20, latest loss 0.40119469165802\n",
      "Finished epoch 21, latest loss 0.40824806690216064\n",
      "Finished epoch 22, latest loss 0.41716450452804565\n",
      "Finished epoch 23, latest loss 0.40353554487228394\n",
      "Finished epoch 24, latest loss 0.400420606136322\n",
      "Finished epoch 25, latest loss 0.39503106474876404\n",
      "Finished epoch 26, latest loss 0.38594016432762146\n",
      "Finished epoch 27, latest loss 0.390238881111145\n",
      "Finished epoch 28, latest loss 0.38933631777763367\n",
      "Finished epoch 29, latest loss 0.36606675386428833\n",
      "Finished epoch 30, latest loss 0.379303902387619\n",
      "Finished epoch 31, latest loss 0.38353416323661804\n",
      "Finished epoch 32, latest loss 0.3865492045879364\n",
      "Finished epoch 33, latest loss 0.3754594027996063\n",
      "Finished epoch 34, latest loss 0.3747345507144928\n",
      "Finished epoch 35, latest loss 0.37382134795188904\n",
      "Finished epoch 36, latest loss 0.35064345598220825\n",
      "Finished epoch 37, latest loss 0.36016398668289185\n",
      "Finished epoch 38, latest loss 0.35942235589027405\n",
      "Finished epoch 39, latest loss 0.3483612537384033\n",
      "Finished epoch 40, latest loss 0.3536302447319031\n",
      "Finished epoch 41, latest loss 0.34399160742759705\n",
      "Finished epoch 42, latest loss 0.3417728543281555\n",
      "Finished epoch 43, latest loss 0.3438285291194916\n",
      "Finished epoch 44, latest loss 0.33743003010749817\n",
      "Finished epoch 45, latest loss 0.34328562021255493\n",
      "Finished epoch 46, latest loss 0.34311211109161377\n",
      "Finished epoch 47, latest loss 0.3408523201942444\n",
      "Finished epoch 48, latest loss 0.34269267320632935\n",
      "Finished epoch 49, latest loss 0.33845970034599304\n",
      "Finished epoch 50, latest loss 0.3349927067756653\n",
      "Finished epoch 51, latest loss 0.331922709941864\n",
      "Finished epoch 52, latest loss 0.34160685539245605\n",
      "Finished epoch 53, latest loss 0.33330023288726807\n",
      "Finished epoch 54, latest loss 0.339871346950531\n",
      "Finished epoch 55, latest loss 0.3381270468235016\n",
      "Finished epoch 56, latest loss 0.3340453505516052\n",
      "Finished epoch 57, latest loss 0.3320385813713074\n",
      "Finished epoch 58, latest loss 0.33285748958587646\n",
      "Finished epoch 59, latest loss 0.32971876859664917\n",
      "Finished epoch 60, latest loss 0.32875770330429077\n",
      "Finished epoch 61, latest loss 0.3307405710220337\n",
      "Finished epoch 62, latest loss 0.32794395089149475\n",
      "Finished epoch 63, latest loss 0.3285735845565796\n",
      "Finished epoch 64, latest loss 0.3270949721336365\n",
      "Finished epoch 65, latest loss 0.3266935646533966\n",
      "Finished epoch 66, latest loss 0.3273366689682007\n",
      "Finished epoch 67, latest loss 0.3278796374797821\n",
      "Finished epoch 68, latest loss 0.32447192072868347\n",
      "Finished epoch 69, latest loss 0.32445088028907776\n",
      "Finished epoch 70, latest loss 0.3264544904232025\n",
      "Finished epoch 71, latest loss 0.32876166701316833\n",
      "Finished epoch 72, latest loss 0.3220057487487793\n",
      "Finished epoch 73, latest loss 0.32260480523109436\n",
      "Finished epoch 74, latest loss 0.3209612965583801\n",
      "Finished epoch 75, latest loss 0.3191836476325989\n",
      "Finished epoch 76, latest loss 0.3157240152359009\n",
      "Finished epoch 77, latest loss 0.3193129003047943\n",
      "Finished epoch 78, latest loss 0.32078203558921814\n",
      "Finished epoch 79, latest loss 0.3196102976799011\n",
      "Finished epoch 80, latest loss 0.31666040420532227\n",
      "Finished epoch 81, latest loss 0.3155297040939331\n",
      "Finished epoch 82, latest loss 0.3168613612651825\n",
      "Finished epoch 83, latest loss 0.31787702441215515\n",
      "Finished epoch 84, latest loss 0.3200445771217346\n",
      "Finished epoch 85, latest loss 0.3142107129096985\n",
      "Finished epoch 86, latest loss 0.3172902464866638\n",
      "Finished epoch 87, latest loss 0.3156742751598358\n",
      "Finished epoch 88, latest loss 0.3153654634952545\n",
      "Finished epoch 89, latest loss 0.31856071949005127\n",
      "Finished epoch 90, latest loss 0.31694328784942627\n",
      "Finished epoch 91, latest loss 0.31382301449775696\n",
      "Finished epoch 92, latest loss 0.3090060353279114\n",
      "Finished epoch 93, latest loss 0.3198185861110687\n",
      "Finished epoch 94, latest loss 0.3186356723308563\n",
      "Finished epoch 95, latest loss 0.31422948837280273\n",
      "Finished epoch 96, latest loss 0.3100944757461548\n",
      "Finished epoch 97, latest loss 0.3158370852470398\n",
      "Finished epoch 98, latest loss 0.3095729351043701\n",
      "Finished epoch 99, latest loss 0.31815874576568604\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 100\n",
    "batch_size = 10\n",
    "\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    for i in range(0, len(X), batch_size):\n",
    "        Xbatch = X[i:i+batch_size]\n",
    "        y_pred = model(Xbatch)\n",
    "        Ybatch = Y[i:i+batch_size]\n",
    "        loss = loss_fn(y_pred, Ybatch)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f'Finished epoch {epoch}, latest loss {loss}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe3d310c",
   "metadata": {},
   "source": [
    "# Evaluate the Model\n",
    "\n",
    "We have trained the NN on the entire dataset. Now, we can evaulate the performance of the network on the same dataset. This doesn't tell you how well the algorithm might perform on new data, only the modeled dataset. You could seperate your data in train and test datasets for that purpose. \n",
    "\n",
    "You will generate predictions for each input, but then you still need to compute a score for the evaluation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "eb558315",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 0.7669270634651184\n"
     ]
    }
   ],
   "source": [
    "#compute accuracy \n",
    "with torch.no_grad():\n",
    "    y_pred = model(X)\n",
    "\n",
    "accuracy = (y_pred.round() == Y).float().mean()\n",
    "print(f'Accuracy {accuracy}')\n",
    "\n",
    "#the round function rounds off the floating point to nearest integer.\n",
    "#== comparess and returns Boolean tensor\n",
    "#mean() provides the count of the number of 1's (pred matches label) divided by number of samples\n",
    "#no_grad() context is optional but suggested, so you relieve y_pred from remembering how it comes up with the number"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neural",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
